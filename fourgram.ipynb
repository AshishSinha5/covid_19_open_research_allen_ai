{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ashis\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langid in c:\\users\\ashis\\anaconda3\\envs\\tf\\lib\\site-packages (1.1.6)\n",
      "Requirement already satisfied: numpy in c:\\users\\ashis\\anaconda3\\envs\\tf\\lib\\site-packages (from langid) (1.18.5)\n",
      "Requirement already satisfied: in_place in c:\\users\\ashis\\anaconda3\\envs\\tf\\lib\\site-packages (0.4.0)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import fileinput\n",
    "import collections\n",
    "import nltk\n",
    "from nltk import ngrams\n",
    "import pickle\n",
    "import json\n",
    "import csv\n",
    "import operator\n",
    "import random\n",
    "nltk.download('punkt')\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.tokenize.punkt import PunktSentenceTokenizer, PunktParameters\n",
    "#import spacy\n",
    "#from spacy_langdetect import LanguageDetector\n",
    "#nlp = spacy.load('en')\n",
    "#nlp.add_pipe(LanguageDetector(), name='language_detector', last=True)\n",
    "\n",
    "! pip install langid\n",
    "import langid\n",
    "\n",
    "import gc\n",
    "\n",
    "! pip install in_place\n",
    "import in_place"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_fourgram_model(file_path):\n",
    "    fourgram_model = collections.defaultdict(lambda: collections.defaultdict(lambda: 0))\n",
    "    with open(file_path, 'r', encoding = 'utf-8') as f:\n",
    "        print('building fourgram frequency dictionary')\n",
    "        for l in tqdm(f):\n",
    "            for w1, w2, w3,w4 in ngrams(l.split(' '), 4, pad_right=True, pad_left=True):\n",
    "                fourgram_model[(w1, w2, w3)][w4] += 1\n",
    "        print('Normalizing frequncies to create probability distribution')\n",
    "        for w1_w2_w3 in tqdm(fourgram_model):\n",
    "            total_count = float(sum(fourgram_model[w1_w2_w3].values()))\n",
    "            for w3 in fourgram_model[w1_w2_w3]:\n",
    "                fourgram_model[w1_w2_w3][w4] /= total_count\n",
    "    return fourgram_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1896it [00:00, 18822.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building fourgram frequency dictionary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3231063it [11:20, 1691.96it/s] "
     ]
    }
   ],
   "source": [
    "train_file = 'data/train_preprocessed.txt'\n",
    "fourram_model = build_fourgram_model(train_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf]",
   "language": "python",
   "name": "conda-env-tf-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
