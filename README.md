## Language Model of Covid-19 data by Allen Ai - (CORD-19)

The project is a part of ongoing **NLP** course at [Chennai Mathematical Institute](https://www.cmi.ac.in/teaching/msc-data-science/index.html).

#### Status: Active

## Objective
The purpose of this project is to build a **language model/pseudo research content** for COVID-19 texual data of over 55,000 documents. We can further go on to do knowledge discovery on the preprossesed corpus to answer questions such risk factors of covid disease, information about the vaccines, origin of virus, etc. Many of these questions are suitable for text mining and we aim to build text mining tools to provide insights on these questions.

## Dataset

We have used partial data set, sourced from Kaggle [COVID-19 Open Research Dataset Challenge (CORD-19)](https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge) contains research articles related to various specializations related to COVID-19.
Download the dataset from [here](https://drive.google.com/drive/folders/1f2pSuVT2cU8NGTY5c4mtPihgyIZYF__m) and directory structure would look as follows

<pre><code>
|--data
    |--json_directory
        --file_1.json
        --file_2.json
        --...
--preprocess.py
--...
</code></pre>

## Getting Started

1. Clone this repo to your local machine (for help see this [tutorial](https://help.github.com/articles/cloning-a-repository/)).
2. Add data files as explained above
3. Extract Relevant data from json - 
<pre><code>
python extract_data.py
</code></pre>
4. Preprocess the text corpus - 
<pre><code>
python preprocess_data.py
</pre></code>

## Preprocessing Steps

- Information extracted from Initial Corpus
  - paper ID
  - paper title
  - abstract text (only text field)
  - body text without bib references without section info
- Preprocessing Steps on the extracted corpus
  - remove foreign language articles
  - remove everything inside brackets
  - remove extra scpaces and urls
  - remove ciatations and figure references
  - replace words using replacement dictionary
  - remove all special characters except "."
  - remove standalone numbers and decimal numbers

## Initial Token Analysis
> Number of Sentences - 7644941 <br>
> Number of words  = 204224987 <br>
> Vocabulary Size = 2097602 <br>
- Heaps' law also seems to hold  for our preprocessed corpus

<p align="center">
  <img width="460" height="300" src="https://github.com/AshishSinha5/covid_19_open_research_allen_ai/blob/master/plots/heaps_law.png">
</p>

## N-gram Language Model

Now that we have our preprocessed corpus we can go ahead and build language models on top of it. We'll focus on one of the most basic yet powerful **n-gram models**. Given a set of token/wordd (w1, w2, w3, ..., w(n-1)), the model finds the next most probable word wn by computing the conditional probability P[wn|w1,w2,w3,...,w(n-1)].

### Trigram

Given word w1,w2 we predict probability distribution of w3 using P(w3|w1,w2) = count(w1,w2,w3)/count(w1,w2) <br>
An example of random sentence generated by trigram model
> the time of the virus multiply quickly in an n antigencapture assays detected a significantly higher level of depression

### Fourgram Model

Given word w1,w2,w3 we predict probability distribution of w4 using P(w4|w1,w2,w3) = count(w1,w2,w3,w4)/count(w1,w2,w3) <br>
An example of random sentence generated by fourgram model
> the latter recommendation is based on a cohort of hospitalized hypertensive covid19 patients controlling for rhv

### Model Evaluation

The best language model is one that best predicts an unseen test set. We'll use the metric called Preplexity to quantify the performance of the model, for a given test sentence preplexity for an n-gram model is calculated as follows - 

<p align="center">
  <img width="460" height="300" src="https://github.com/AshishSinha5/covid_19_open_research_allen_ai/blob/master/figures/perplexity.PNG">
</p>

|Model|Preplexity|
|-------|---------|
|Trigram|103.18|
|Fourgram|83.22|

