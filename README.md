## Language Model of Covid-19 data by Allen Ai - (CORD-19)

The project is a part of ongoing **NLP** course at [Chennai Mathematical Institute](https://www.cmi.ac.in/teaching/msc-data-science/index.html).

#### Status: Active

## Objective
The purpose of this project is to build a **language model/pseudo research content** for COVID-19 texual data of over 55,000 documents. We can further go on to do knowledge discovery on the preprossesed corpus to answer questions such risk factors of covid disease, information about the vaccines, origin of virus, etc. Many of these questions are suitable for text mining and we aim to build text mining tools to provide insights on these questions.

## Dataset

We have used partial data set, sourced from Kaggle [COVID-19 Open Research Dataset Challenge (CORD-19)](https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge) contains research articles related to various specializations related to COVID-19.
Download the dataset from [here](https://drive.google.com/drive/folders/1f2pSuVT2cU8NGTY5c4mtPihgyIZYF__m) and directory structure would look as follows

<pre><code>
|--data
    |--json_directory
        --file_1.json
        --file_2.json
        --...
--preprocess.py
--...
</code></pre>

## Getting Started

1. Clone this repo to your local machine (for help see this [tutorial](https://help.github.com/articles/cloning-a-repository/)).
2. Add data files as explained above
3. Extract Relevant data from json - 
<pre><code>
python extract_data.py
</code></pre>
4. Preprocess the text corpus - 
<pre><code>
python preprocess_data.py
</code></pre>
5. Create the fourgram and trigram model and calculate evaluation metric - 
<pre><code>
python ngram_model.py
</code></pre>


## Preprocessing Steps

- Information extracted from Initial Corpus
  - paper ID
  - paper title
  - abstract text (only text field)
  - body text without bib references without section info
- Preprocessing Steps on the extracted corpus
  - remove foreign language articles
  - remove everything inside brackets
  - remove extra scpaces and urls
  - remove ciatations and figure references
  - replace words using replacement dictionary
  - remove all special characters except "."
  - remove standalone numbers and decimal numbers

## Initial Token Analysis
> Number of Sentences - 7644941 <br>
> Number of words  = 204224987 <br>
> Vocabulary Size = 2097602 <br>
- Heaps' law also seems to hold  for our preprocessed corpus

<p align="center">
  <img width="460" height="300" src="https://github.com/AshishSinha5/covid_19_open_research_allen_ai/blob/master/plots/heaps_law.png">
</p>

## N-gram Language Model

Now that we have our preprocessed corpus we can go ahead and build language models on top of it. We'll focus on one of the most basic yet powerful **n-gram models**. Given a set of token/wordd (w1, w2, w3, ..., w(n-1)), the model finds the next most probable word wn by computing the conditional probability P[wn|w1,w2,w3,...,w(n-1)].

### Trigram

Given word w1,w2 we predict probability distribution of w3 using P(w3|w1,w2) = count(w1,w2,w3)/count(w1,w2) <br>
An example of random sentence generated by trigram model
> the time of the virus multiply quickly in an n antigencapture assays detected a significantly higher level of depression

### Fourgram Model

Given word w1,w2,w3 we predict probability distribution of w4 using P(w4|w1,w2,w3) = count(w1,w2,w3,w4)/count(w1,w2,w3) <br>
An example of random sentence generated by fourgram model
> the latter recommendation is based on a cohort of hospitalized hypertensive covid19 patients controlling for rhv

### Model Evaluation

The best language model is one that best predicts an unseen test set. We'll use the metric called Preplexity to quantify the performance of the model, for a given test sentence preplexity for an n-gram model is calculated as follows - 

<p align="center">
  <img width="460" height="300" src="https://github.com/AshishSinha5/covid_19_open_research_allen_ai/blob/master/figures/perplexity.PNG">
</p>

|Model|Preplexity|
|-------|---------|
|Trigram|103.18|
|Fourgram|83.22|

We can see that the perplexity value of fourgram model is less than that of trigram, indicating that the fourgram model 
performs better than the trigram model on the test set.

## Word Similarity and Knowledge Graph

As the amount of literature grows it becomes increasingly harder to make a new discovery, concepts become more scattered,
especially when there is growing number of scientific journals and readily available research content on the internet.<br>

Reading all that stuff and filtering unimportant information is almost impossible for a single 
individual. If only we could have a system that can look at all this data globally, find hidden connections/relations and
present those findings to us. We have **Word2Vec** to our rescue. 

Word2vec takes as its input a large corpus of text and produces a vector space, typically of several hundred dimensions,
with each unique word in the corpus being assigned a corresponding vector in the space. Word vectors are positioned in 
the vector space such that words that share common contexts in the corpus are located in close proximity to one another 
in the space. We'll see how we can discover relationships between words in our corpus

### Preprocessing 

Before we can run Word2Vec algorithm on our test corpus we would need to pre-process it. We take the following steps - 

1. **Remove Stopwords** -  as they do not provide any extra information for the word2vec model and word similarity
2. **Lemmatization** - words like “are, is, being” and plurals, etc. are transformed “be”, signular and so on.  
3. **Remove Small sentences** -  sentences of less than 5 tokens(words) as not considered in making word vectors

### Computing Word Vectors

Running the algorithm from **gensim** package on 8194615 sentences containing 106333038 raw words resulted in 836047 
unique word vectors (read vocabulary) of dimension 100. Extracting the top 5 most similar words for some of the tokens 
gave the following results.

1. Source word - "fever" <br>
   Results - ['fever', 'pyrexia', 'chill', 'febrile', 'paroxysm', 'myalgia'] <br>
   We get words relating to fever and muscle pain, which are reseonably similar to the word fever.
   
2. Source word - "fatal" <br>
   Results - ['fatal', 'severe', 'lifethreatening', 'fulminating', 'fulminant', 'lethal']

3. Source word - "antigen" <br>
   Results - ['antigen', 'autoantigens', 'apc', 'antibody', 'glycoprotein', 'selfantigens']

4. Source word - "corona" <br>
   Results - ['corona', 'coronavirus', 'oronavirus', 'borna', 'ebola', 'deadly']

### Knowledge Graph on a set of tokens

The next important piece of the puzzle is to find how well a keyword is connected with other keywords. Here, we want to 
use the Graph to structure the connected keywords. It allows us to represent the connections in a structured way. It is 
also possible to filter the connections by studying the edges of the nodes (representing the relationship of the keyword) 
and the nodes representing the keywords. We'll use networkx library to construct our graph.

Full graph can be accessed from [here](https://drive.google.com/file/d/1qW9aUbv3-PKf6le0wzytFwPqu2lXaCTd/view?usp=sharing).
Some of the areas of graphs are shown below - 


## Charecter Level RNN

TODO
