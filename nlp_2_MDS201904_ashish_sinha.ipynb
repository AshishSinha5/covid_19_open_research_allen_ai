{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLP Assignment 2, Language Model\n",
    "## Ashish Kumar Sinha\n",
    "## MDS201904"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ashis\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langid in c:\\users\\ashis\\anaconda3\\envs\\tf\\lib\\site-packages (1.1.6)\n",
      "Requirement already satisfied: numpy in c:\\users\\ashis\\anaconda3\\envs\\tf\\lib\\site-packages (from langid) (1.18.5)\n",
      "Requirement already satisfied: in_place in c:\\users\\ashis\\anaconda3\\envs\\tf\\lib\\site-packages (0.4.0)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import fileinput\n",
    "import collections\n",
    "import nltk\n",
    "from nltk import ngrams\n",
    "import pickle\n",
    "import json\n",
    "import csv\n",
    "import operator\n",
    "import random\n",
    "import math\n",
    "from numba import jit, cuda\n",
    "from IPython.display import Image\n",
    "nltk.download('punkt')\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.tokenize.punkt import PunktSentenceTokenizer, PunktParameters\n",
    "#import spacy\n",
    "#from spacy_langdetect import LanguageDetector\n",
    "#nlp = spacy.load('en')\n",
    "#nlp.add_pipe(LanguageDetector(), name='language_detector', last=True)\n",
    "\n",
    "! pip install langid\n",
    "import langid\n",
    "\n",
    "import gc\n",
    "\n",
    "! pip install in_place\n",
    "import in_place"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'data/initial_corpus.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing spaces before \".\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_spaces(file_path):\n",
    "    \"\"\"\n",
    "    args: file_path - file path of the corpus\n",
    "    returns: converts text to lower case\n",
    "    \"\"\" \n",
    "    print('---------------------------------------------------------')\n",
    "    print(\"removing exptra spaces\")\n",
    "    with in_place.InPlace(file_path, encoding = 'utf-8') as file:\n",
    "        for line in tqdm(file):  \n",
    "            line = re.sub(r'\\s+\\.', '. ', line)\n",
    "            line = re.sub(r'(\\.)(\\w+)',r'\\1 \\2', line)\n",
    "            file.write(line)\n",
    "    gc.collect()\n",
    "    print('Done')\n",
    "    print('---------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "51it [00:00, 491.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------\n",
      "removing exptra spaces\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "52817it [01:59, 441.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "---------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "remove_spaces(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Tokenizing and writing one sentence per line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_tokenize(file_path):\n",
    "    \"\"\"\n",
    "    args: file_path - file path of text corpus\n",
    "    returns: writes one sentence per line\n",
    "    \"\"\"\n",
    "    print('---------------------------------------------------------')\n",
    "    print(\"sentence tokenizing\")\n",
    "    pat = re.compile(r'([A-Z][^\\.!?]*[\\.!?])', re.M)\n",
    "    with open('data/preprocessed.txt', 'w', encoding='utf-8') as f2:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f1:\n",
    "            for i,line in tqdm(enumerate(f1)):\n",
    "                # print(line)\n",
    "                sentences = sent_tokenize(line)\n",
    "                # print(sentences)\n",
    "                for sentence in sentences:\n",
    "                    f2.write(sentence)\n",
    "                    f2.write('\\n')\n",
    "                    \n",
    "    print('Done')\n",
    "    print('---------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------\n",
      "sentence tokenizing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "52817it [04:44, 185.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "---------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "sentence_tokenize(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting preprocessed corpus to lower case, removing ending punctuation ('.', '?', etc) and delimiting sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_lower(file_path):\n",
    "    \"\"\"\n",
    "    args: file_path - file path of the corpus\n",
    "    returns: converts text to lower case\n",
    "    \"\"\"\n",
    "    print('---------------------------------------------------------')\n",
    "    print(\"converting to lower case and adding delimiters\")\n",
    "    with in_place.InPlace(file_path, encoding = 'utf-8') as file:\n",
    "        for line in tqdm(file):  \n",
    "            if len(line) > 1:\n",
    "                line = line.lower()\n",
    "                file.write(line[:-2])\n",
    "                file.write('\\n')\n",
    "    gc.collect()\n",
    "    print('Done')\n",
    "    print('---------------------------------------------------------')\n",
    "    with in_place.InPlace(file_path, encoding = 'utf-8') as file:\n",
    "        for line in tqdm(file):  \n",
    "            if len(line) > 50:\n",
    "                file.write(line)\n",
    "                #file.write('\\n')\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'data/preprocessed.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "41653it [00:00, 413469.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------\n",
      "converting to lower case and adding delimiters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9039055it [00:20, 450860.40it/s]\n",
      "57465it [00:00, 570483.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "---------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9039055it [00:15, 589976.23it/s]\n"
     ]
    }
   ],
   "source": [
    "to_lower(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the sentences after preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multifaceted covid19 russian experience according to current live statistics at the time of editing this letter russia has been the third country in the world to be affected by covid19 with both new cases and death rates rising\n",
      "\n",
      "it remains in a position of advantage due to the later onset of the viral spread within the country since the worldwide disease outbreak\n",
      "\n",
      "the first step in fighting the epidemic was nationwide lock down on march th\n",
      "\n",
      "most of the multidisciplinary hospitals have been repurposed as dedicated covid19 centres so the surgeons started working as infectious disease specialists\n",
      "\n",
      "such a reallocation of health care capacity results in the effective management of this epidemiological problem\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('data/preprocessed.txt', 'r') as f:\n",
    "    for i in range(5):\n",
    "        print(next(f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Test Split\n",
    "\n",
    "**The corpus containes more that 83L sentences due to computaional limitations I only use portion of data to build the language model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test(file_path, corpus_prop = 0.5, train_prop = 0.9):\n",
    "    \"\"\"\n",
    "    args : file_path : path of file for preprocessed corpus\n",
    "    returns : splits raw text to train and test set\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    1 - count the number of lines\n",
    "    2 - |train_set| = train_prop * number_of_lines\n",
    "    3 - |test_set| = (1 - train_prop) * number_lines\n",
    "    \"\"\"\n",
    "    \n",
    "    train_file = 'data/train_preprocessed.txt'\n",
    "    test_file = 'data/test_preprocessed.txt'\n",
    "    print('getting line count')\n",
    "    with open(file_path, 'r', encoding = 'utf-8') as f:\n",
    "        for i, l in tqdm(enumerate(f)):\n",
    "            pass\n",
    "    num_lines =  i + 1\n",
    "    print('Number of lines = {}'.format(num_lines))\n",
    "    print('building train, test set using only {} % of the entire corpus'.format(corpus_prop*100))\n",
    "    f1 = open(train_file, 'w', encoding = 'utf-8')\n",
    "    f2 = open(test_file, 'w', encoding = 'utf-8')\n",
    "    k = 0\n",
    "    with open(file_path, 'r', encoding = 'utf-8') as f:\n",
    "        for i, l in tqdm(enumerate(f)):\n",
    "            if i < (num_lines*corpus_prop):\n",
    "                k += 1\n",
    "                if k < (num_lines*corpus_prop*train_prop):\n",
    "                    f1.write(l)\n",
    "                else:\n",
    "                    f2.write(l)\n",
    "    with open(train_file, 'r', encoding = 'utf-8') as f:\n",
    "        for i, l in tqdm(enumerate(f)):\n",
    "            pass\n",
    "    print(\"Number of sentences in train set = {}\".format(i+1))\n",
    "    with open(test_file, 'r', encoding = 'utf-8') as f:\n",
    "        for i, l in tqdm(enumerate(f)):\n",
    "            pass\n",
    "    print(\"Number of sentences in test set = {}\".format(i+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "102639it [00:00, 1019022.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting line count\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8374537it [00:07, 1076872.17it/s]\n",
      "49102it [00:00, 487461.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of lines = 8374537\n",
      "building train, test set using only 50.0 % of the entire corpus\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8374537it [00:12, 691273.44it/s] \n",
      "3349782it [00:02, 1210793.59it/s]\n",
      "238525it [00:00, 1192853.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences in train set = 3349782\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "837455it [00:00, 1203000.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences in test set = 837455\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "file_path = 'data/preprocessed.txt'\n",
    "split_train_test(file_path, train_prop=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trigram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_trigram_model(file_path):\n",
    "    trigram_model = collections.defaultdict(lambda: collections.defaultdict(lambda: 0))\n",
    "    with open(file_path, 'r', encoding = 'utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "        print('building trigram frequency dictionary')\n",
    "        for l in tqdm(lines):\n",
    "            for w1, w2, w3 in ngrams(l.split(' '),3, pad_right=True, pad_left=True):\n",
    "                trigram_model[(w1, w2)][w3] += 1\n",
    "        print('Normalizing frequncies to create probability distribution')\n",
    "        for w1_w2 in tqdm(trigram_model):\n",
    "            total_count = float(sum(trigram_model[w1_w2].values()))\n",
    "            for w3 in trigram_model[w1_w2]:\n",
    "                trigram_model[w1_w2][w3] /= total_count\n",
    "    return trigram_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                        | 1078/3349814 [00:00<05:12, 10702.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building trigram frequency dictionary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████| 3349814/3349814 [01:51<00:00, 30096.19it/s]\n",
      "  0%|                                                                         | 591/10460569 [00:00<52:54, 3294.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizing frequncies to create probability distribution\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████| 10460569/10460569 [00:16<00:00, 649805.95it/s]\n"
     ]
    }
   ],
   "source": [
    "train_file = 'data/train_preprocessed.txt'\n",
    "trigram_model = build_trigram_model(train_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|███████████████████████████████████████████████████████████████▌  | 16568902/17220321 [03:10<00:03, 202692.78it/s]"
     ]
    }
   ],
   "source": [
    "with open('data/trigram.csv', 'w', encoding ='utf-8') as f:\n",
    "    writer = csv.writer(f)    \n",
    "    writer.writerow(['w1_w2', 'w3_prob'])\n",
    "    for k,v in tqdm(trigram_model.items()):\n",
    "        l = [k]\n",
    "        l.append(list(v.items()))\n",
    "        writer.writerow(l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding most frequent starting word and generating a ramdom sentence from it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_frequent_word =  max(trigram_model[None, None].items(), key=operator.itemgetter(1))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_frequent_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Randomly generated sentence from the model\n",
      "the time of the virus multiply quickly in an n antigencapture assays detected a significantly higher level of depression\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = [None, most_frequent_word]\n",
    "sentence_finished = False\n",
    "while not sentence_finished:\n",
    "    # select a random probability threshold  \n",
    "    r = random.random()\n",
    "    accumulator = .0\n",
    "\n",
    "    for word in trigram_model[tuple(text[-2:])].keys():\n",
    "        accumulator += trigram_model[tuple(text[-2:])][word]\n",
    "        # select words that are above the probability threshold\n",
    "        if accumulator >= r:\n",
    "            text.append(word)\n",
    "            break\n",
    "\n",
    "    if text[-2:] == [None, None]:\n",
    "        sentence_finished = True\n",
    "\n",
    "print('Randomly generated sentence from the model')\n",
    "print (' '.join([t for t in text if t]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fourgram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_fourgram_model(file_path):\n",
    "    fourgram_model = collections.defaultdict(lambda: collections.defaultdict(lambda: 0))\n",
    "    with open(file_path, 'r', encoding = 'utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "        print('building fourgram frequency dictionary')\n",
    "        for i, l in tqdm(enumerate(lines)):\n",
    "            for w1, w2, w3, w4 in ngrams(l.split(' '), 4, pad_right=True, pad_left=True):\n",
    "                fourgram_model[(w1, w2, w3)][w4] += 1\n",
    "            #if i == 100:\n",
    "            #    break\n",
    "        # print(list(fourgram_model.keys())[0])\n",
    "        print('Normalizing frequncies to create probability distribution')\n",
    "        for w1_w2_w3 in fourgram_model:\n",
    "            total_count = float(sum(fourgram_model[w1_w2_w3].values()))\n",
    "            for w4 in fourgram_model[w1_w2_w3]:\n",
    "                fourgram_model[w1_w2_w3][w4] /= total_count\n",
    "    return fourgram_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "141it [00:00, 1017.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building fourgram frequency dictionary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3349814it [20:26, 2730.39it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizing frequncies to create probability distribution\n"
     ]
    }
   ],
   "source": [
    "train_file = 'data/train_preprocessed.txt'\n",
    "fourgram_model = build_fourgram_model(train_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding most frequent starting word and generating a ramdom sentence from it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_frequent_word_4gram =  max(fourgram_model[None, None, None].items(), key=operator.itemgetter(1))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_frequent_word_4gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Randomly generated sentence from the 4gram model\n",
      "the latter recommendation is based on a cohort of hospitalized hypertensive covid19 patients controlling for rhv\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = [None, None, most_frequent_word_4gram]\n",
    "sentence_finished = False\n",
    "while not sentence_finished:\n",
    "    # select a random probability threshold  \n",
    "    r = random.random()\n",
    "    accumulator = .0\n",
    "\n",
    "    for word in fourgram_model[tuple(text[-3:])].keys():\n",
    "        accumulator += fourgram_model[tuple(text[-3:])][word]\n",
    "        # select words that are above the probability threshold\n",
    "        if accumulator >= r:\n",
    "            text.append(word)\n",
    "            break\n",
    "\n",
    "    if text[-3:] == [None, None, None]:\n",
    "        sentence_finished = True\n",
    "\n",
    "print('Randomly generated sentence from the 4gram model')\n",
    "print (' '.join([t for t in text if t]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating Perplexities to evaluate models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For evaluating the our models we'll use **perplexity metric**, given a sentence containing (w1, w2, w3, w4, ...), perplexity can be calculated as - "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/perplexity.png\">, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**There for maximizing probability is the same as minimizing perplexity**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "from nltk.lm import MLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_text = []\n",
    "with open('data/train_preprocessed.txt', 'r', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "    tokenized_text = [list(line[:-1].split(' ')) for line in lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 3\n",
    "train_data_trigrams, padded_vocab = padded_everygram_pipeline(n, tokenized_text)\n",
    "trigram_model = MLE(n)\n",
    "trigram_model.fit(train_data_trigrams, padded_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_text_test = []\n",
    "with open('data/test_preprocessed.txt', 'r', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "    tokenized_text_test = [list(line[:-1].split(' ')) for i, line in enumerate(lines) if (i < 50000 )]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenized_text_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_trigrams, _ = padded_everygram_pipeline(n, tokenized_text_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50000it [00:24, 2049.44it/s]\n"
     ]
    }
   ],
   "source": [
    "perplexity = 0\n",
    "n_valid = 0\n",
    "for i, test in tqdm(enumerate(test_data_trigrams)):\n",
    "    try:\n",
    "        p = trigram_model.perplexity(test)\n",
    "        if p > 0 and (not math.isinf(p)):\n",
    "            perplexity += p\n",
    "            n_valid += 1\n",
    "    except ZeroDevideError:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average perplexity trigrams over 50000 valid test sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "103.18152347921301"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perplexity/n_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n1 = 4\n",
    "train_data_fourgrams, padded_vocab = padded_everygram_pipeline(n1, tokenized_text)\n",
    "fourgram_model = MLE(n1)\n",
    "fourgram_model.fit(train_data_fourgrams, padded_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_fourgrams, _ = padded_everygram_pipeline(n1, tokenized_text_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perplexity = 0\n",
    "n_valid = 0\n",
    "for i, test in tqdm(enumerate(test_data_fourggrams)):\n",
    "    try:\n",
    "        p = fourgram_model.perplexity(test)\n",
    "        if p > 0 and (not math.isinf(p)):\n",
    "            perplexity += p\n",
    "            n_valid += 1\n",
    "    except ZeroDevideError:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average perplexity for fourgram model for 50000 valid test sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83.2184359435987"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perplexity/n_valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that perplexity of fourgram model is lower than that of trigram model, hence fourgram model performs better on the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 -\n",
    "An effitient way to handle large set of parameters is to use better datastructure with lower space complexity such as hash sets, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7 -\n",
    "\n",
    "While counting the frequencies of the last word in ngram model we can parallel proces that set, using hadoop clusters mainly by employing a technique known as map-reduce which is highly efficient in parallal processing and ditributing tasks over several systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf]",
   "language": "python",
   "name": "conda-env-tf-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
